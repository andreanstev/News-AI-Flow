{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9341526,"sourceType":"datasetVersion","datasetId":5661278}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-06T09:09:52.861095Z","iopub.execute_input":"2024-09-06T09:09:52.861373Z","iopub.status.idle":"2024-09-06T09:09:53.222254Z","shell.execute_reply.started":"2024-09-06T09:09:52.861340Z","shell.execute_reply":"2024-09-06T09:09:53.221253Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Step 1: Install dependencies\n!pip install transformers datasets evaluate huggingface_hub tqdm torch","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:06:27.265735Z","iopub.execute_input":"2024-09-08T01:06:27.266468Z","iopub.status.idle":"2024-09-08T01:06:42.967701Z","shell.execute_reply.started":"2024-09-08T01:06:27.266422Z","shell.execute_reply":"2024-09-08T01:06:42.966611Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.24.6)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments, pipeline\nfrom datasets import load_dataset\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, AdamW\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport torch\nfrom transformers import BertTokenizer, BertForQuestionAnswering, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers.data.processors.squad import SquadProcessor\nfrom transformers import squad_convert_examples_to_features\nfrom transformers import AutoTokenizer\nfrom tqdm.auto import tqdm\nimport evaluate\nimport collections\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:06:48.931177Z","iopub.execute_input":"2024-09-08T01:06:48.931614Z","iopub.status.idle":"2024-09-08T01:06:51.079675Z","shell.execute_reply.started":"2024-09-08T01:06:48.931547Z","shell.execute_reply":"2024-09-08T01:06:51.078841Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from datasets import load_from_disk\n\n# Load DatasetDict from Google Drive\ndataset_dict_path = '/kaggle/input/news-qa-data/news-qa-data'\ndatasets = load_from_disk(dataset_dict_path)\n\n# Verify the content\nprint(datasets)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:06:53.749823Z","iopub.execute_input":"2024-09-08T01:06:53.750603Z","iopub.status.idle":"2024-09-08T01:06:54.073035Z","shell.execute_reply.started":"2024-09-08T01:06:53.750561Z","shell.execute_reply":"2024-09-08T01:06:54.072023Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['context', 'question', 'answers', 'id'],\n        num_rows: 57224\n    })\n    validation: Dataset({\n        features: ['context', 'question', 'answers', 'id'],\n        num_rows: 14335\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:06:56.744005Z","iopub.execute_input":"2024-09-08T01:06:56.744796Z","iopub.status.idle":"2024-09-08T01:06:57.942992Z","shell.execute_reply.started":"2024-09-08T01:06:56.744751Z","shell.execute_reply":"2024-09-08T01:06:57.942073Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ac756f561a4122b4bd8a7c3d290a1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0e55a92af44439281525d8af6dc1deb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ca5dc824bf4d38899f9edb3f2c08ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed7339c450f34fafb4f1b0d0407b5e2c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"max_length = 512\nstride = 128\n\ndef preprocess_training_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n\n        # Periksa jika jawaban ada\n        if len(answer[\"answer_start\"]) == 0 or len(answer[\"text\"]) == 0:\n            start_positions.append(0)\n            end_positions.append(0)\n            continue\n\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:06:59.584907Z","iopub.execute_input":"2024-09-08T01:06:59.585326Z","iopub.status.idle":"2024-09-08T01:06:59.598945Z","shell.execute_reply.started":"2024-09-08T01:06:59.585282Z","shell.execute_reply":"2024-09-08T01:06:59.598013Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Pemetaan dataset tanpa caching\ntrain_dataset = datasets[\"train\"].map(\n    preprocess_training_examples,\n    batched=True,\n    remove_columns=datasets[\"train\"].column_names,\n    keep_in_memory=True  # Nonaktifkan caching\n)\n\nlen(datasets[\"train\"]), len(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:07:08.635433Z","iopub.execute_input":"2024-09-08T01:07:08.635841Z","iopub.status.idle":"2024-09-08T01:08:57.214808Z","shell.execute_reply.started":"2024-09-08T01:07:08.635802Z","shell.execute_reply":"2024-09-08T01:08:57.213772Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/57224 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"876380c6580c4f8db2c6b08eceb0f8a0"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(57224, 99693)"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_validation_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:08:57.216612Z","iopub.execute_input":"2024-09-08T01:08:57.216910Z","iopub.status.idle":"2024-09-08T01:08:57.226366Z","shell.execute_reply.started":"2024-09-08T01:08:57.216879Z","shell.execute_reply":"2024-09-08T01:08:57.225282Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"validation_dataset = datasets[\"validation\"].map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=datasets[\"validation\"].column_names,\n    keep_in_memory=True  # Nonaktifkan caching\n)\nlen(datasets[\"validation\"]), len(validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:08:57.227641Z","iopub.execute_input":"2024-09-08T01:08:57.227997Z","iopub.status.idle":"2024-09-08T01:09:35.384740Z","shell.execute_reply.started":"2024-09-08T01:08:57.227954Z","shell.execute_reply":"2024-09-08T01:09:35.383632Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14335 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd036b30526d482a8d2bce35f12f2764"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(14335, 24856)"},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(start_logits, end_logits, features, examples):\n    metric = evaluate.load(\"squad\")\n    n_best = 20  # jumlah best predictions yang ingin diambil\n    max_answer_length = 200  # batasan maksimal panjang jawaban\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    for example in tqdm(examples):\n        example_id = example[\"id\"]\n        context = example[\"context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip answers that are not fully in the context\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # Skip answers with a length that is either < 0 or > max_answer_length\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": str(example_id), \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": str(example_id), \"prediction_text\": \"\"})\n\n    # Format references to match SQuAD evaluation\n    theoretical_answers = [{\"id\": str(ex[\"id\"]), \"answers\": ex[\"answers\"]} for ex in examples]\n\n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T02:18:23.851395Z","iopub.execute_input":"2024-09-08T02:18:23.851980Z","iopub.status.idle":"2024-09-08T02:18:23.867685Z","shell.execute_reply.started":"2024-09-08T02:18:23.851937Z","shell.execute_reply":"2024-09-08T02:18:23.866532Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:09:35.404432Z","iopub.execute_input":"2024-09-08T01:09:35.404861Z","iopub.status.idle":"2024-09-08T01:09:36.861642Z","shell.execute_reply.started":"2024-09-08T01:09:35.404810Z","shell.execute_reply":"2024-09-08T01:09:36.860705Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90823c29c9124f838e04047635b5b7fb"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    \"distilbert-finetuned-newsqa-squad\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    fp16=True,\n    push_to_hub=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:09:36.862929Z","iopub.execute_input":"2024-09-08T01:09:36.863252Z","iopub.status.idle":"2024-09-08T01:09:37.000918Z","shell.execute_reply.started":"2024-09-08T01:09:36.863216Z","shell.execute_reply":"2024-09-08T01:09:36.999883Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T01:09:37.002270Z","iopub.execute_input":"2024-09-08T01:09:37.002630Z","iopub.status.idle":"2024-09-08T02:00:17.913078Z","shell.execute_reply.started":"2024-09-08T01:09:37.002585Z","shell.execute_reply":"2024-09-08T02:00:17.912119Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240908_011049-w92wa1fm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/prasetyo-widyantoro-dwidasa-samsara-indonesia/huggingface/runs/w92wa1fm' target=\"_blank\">distilbert-finetuned-newsqa-squad</a></strong> to <a href='https://wandb.ai/prasetyo-widyantoro-dwidasa-samsara-indonesia/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/prasetyo-widyantoro-dwidasa-samsara-indonesia/huggingface' target=\"_blank\">https://wandb.ai/prasetyo-widyantoro-dwidasa-samsara-indonesia/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/prasetyo-widyantoro-dwidasa-samsara-indonesia/huggingface/runs/w92wa1fm' target=\"_blank\">https://wandb.ai/prasetyo-widyantoro-dwidasa-samsara-indonesia/huggingface/runs/w92wa1fm</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6231' max='6231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6231/6231 49:09, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>3.152300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.377300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.947700</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.834300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.750100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.661100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.650700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.644200</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.589900</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.558900</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.560500</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.508300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6231, training_loss=1.8400946151691768, metrics={'train_runtime': 3039.1506, 'train_samples_per_second': 32.803, 'train_steps_per_second': 2.05, 'total_flos': 1.3025199501490176e+16, 'train_loss': 1.8400946151691768, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions, _, _ = trainer.predict(validation_dataset)\nstart_logits, end_logits = predictions\ncompute_metrics(start_logits, end_logits, validation_dataset, datasets[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-08T02:18:25.692011Z","iopub.execute_input":"2024-09-08T02:18:25.692442Z","iopub.status.idle":"2024-09-08T02:25:33.423625Z","shell.execute_reply.started":"2024-09-08T02:18:25.692402Z","shell.execute_reply":"2024-09-08T02:25:33.422394Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92d1f4d499e749d88f001daa9d9595fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8db289d070044788ad10b949363de7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14335 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a961b0f8a65e4dfd995edf6679fd7a47"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 41.48587373561214, 'f1': 55.77324510831456}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\n\nquestion_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device='cuda')\ncontext = \"\"\"\nAutoML: Automated Machine Learning tools like Auto-sklearn and H2O.ai simplify the process of building machine learning models.\n\"\"\"\nquestion = \"What are some tools for automated machine learning?\"\nquestion_answerer(question=question, context=context)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T02:27:30.155398Z","iopub.execute_input":"2024-09-08T02:27:30.155852Z","iopub.status.idle":"2024-09-08T02:27:30.184010Z","shell.execute_reply.started":"2024-09-08T02:27:30.155810Z","shell.execute_reply":"2024-09-08T02:27:30.182975Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'score': 0.006397548597306013,\n 'start': 47,\n 'end': 68,\n 'answer': 'Auto-sklearn and H2O.'}"},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import HfApi, HfFolder\n\napi = HfApi()\ntoken = \"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\"\nHfFolder.save_token(token)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T02:28:00.387823Z","iopub.execute_input":"2024-09-08T02:28:00.388638Z","iopub.status.idle":"2024-09-08T02:28:00.395585Z","shell.execute_reply.started":"2024-09-08T02:28:00.388595Z","shell.execute_reply":"2024-09-08T02:28:00.394422Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(\"distilbert-uncased-newsqa-squad\")\ntokenizer.push_to_hub(\"distilbert-uncased-newsqa-squad\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T02:30:09.591920Z","iopub.execute_input":"2024-09-08T02:30:09.592783Z","iopub.status.idle":"2024-09-08T02:30:22.133027Z","shell.execute_reply.started":"2024-09-08T02:30:09.592739Z","shell.execute_reply":"2024-09-08T02:30:22.131845Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"242e7467d68a4e6da30a98aaabcba20a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34e6a6f561ed40bd92f3b2f62169aac1"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Prasetyow12/distilbert-uncased-newsqa-squad/commit/51c31c6ad6c8cf01da3e1ece547f84b61fa7a233', commit_message='Upload tokenizer', commit_description='', oid='51c31c6ad6c8cf01da3e1ece547f84b61fa7a233', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# Load model dan tokenizer dari Hugging Face Hub\nmodel_distilbert_qa = AutoModelForQuestionAnswering.from_pretrained(\"Prasetyow12/distilbert-uncased-newsqa-squad\")\ntokenizer_distilbert_qa = AutoTokenizer.from_pretrained(\"Prasetyow12/distilbert-uncased-newsqa-squad\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T02:32:16.458699Z","iopub.execute_input":"2024-09-08T02:32:16.459095Z","iopub.status.idle":"2024-09-08T02:32:21.558751Z","shell.execute_reply.started":"2024-09-08T02:32:16.459059Z","shell.execute_reply":"2024-09-08T02:32:21.557452Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/561 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"613ff90601d44b80ac0cab3e71ae9e47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7fb9c62f2674eaaab94f4ca07af6140"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe3bb653c974e24815e753852279be4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"111dc5e328814addb8006bedbe324b06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"998044357c954bfaa16fd1cb2b8cae8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a08652d5478d436a99a936bbe53408aa"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nimport re\n\n# Load model and tokenizer from Hugging Face\n#model = AutoModelForQuestionAnswering.from_pretrained(\"Prasetyow12/distilbert-uncased-newsqa-finetuned-squad\")\n#tokenizer = AutoTokenizer.from_pretrained(\"Prasetyow12/distilbert-uncased-newsqa-finetuned-squad\")\n\n# Buat pipeline untuk inference\nquestion_answerer = pipeline(\"question-answering\", model=model_distilbert_qa, tokenizer=tokenizer_distilbert_qa)\n\n# Contoh konteks\ncontext = \"\"\"\n(CNN) -- Police are investigating whether or what family issues might have prompted a California man to shoot six of his family members -- killing five -- before committing suicide. His wife was critically wounded. Authorities on Tuesday said Devan Kalathat, 42, shot his family Sunday night at his Santa Clara townhouse, killing two adults and three children. Kalathat killed his 11-year-old son, Akhil Dev; his 4-year-old daughter, Negha Dev; his 35-year-old brother-in-law Ashok Appu Poothemkandi; Poothemkandi's 25-year-old wife, Suchitra Sivaraman; and the couple's 11-month-old daughter, Ahana. Kalathat's 34-year-old wife, who was not identified, sustained multiple gunshot wounds and remains in critical condition, said Lt. Phil Cooke. \"Family dynamics and personal relationships may have played a factor,\" Cooke told reporters Tuesday. He said Kalathat was employed as an engineer and nothing indicated he was facing \"layoff or financial crisis.\" Investigators believe Kalathat used two .45-caliber semi-automatic pistols, both of which he owned. Cooke said Kalathat bought one of the pistols in February and the other nearly two weeks ago -- roughly the same time his wife's brother, Poothemkandi, arrived in California from India with Suchitra Sivaraman and Ahana. Cooke noted that Poothemkandi was an \"educated professional\" with plans to stay in the Bay Area to work on a project for a high-tech firm. Police were called after a neighbor noticed Kalathat's wounded wife outside the home around 8:30 p.m. (11:30 p.m. ET), Cooke said. When police arrived, other victims were found around the kitchen and dining room in what Cooke described as \"a very gruesome scene.\" The family shooting comes just two months after a Los Angeles father who, after he and his wife were fired from their jobs, killed her and their five young children before turning the gun on himself.\n\"\"\"\nquestion = \"What did police say was a factor?\"\n\n# Fungsi untuk membersihkan teks\ndef clean_text(text):\n    # Definisikan pola regex untuk berbagai pembersihan\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+', re.IGNORECASE)\n    hashtag_pattern = re.compile(r'#\\w+', re.IGNORECASE)\n    double_space_pattern = re.compile(r'\\s\\s+')\n    header_pattern = re.compile(r'^.*?--\\s?', re.IGNORECASE)\n    video_pattern = re.compile(r'VIDEO:.*?(?:\\.\\s|$)', re.IGNORECASE)\n\n    # Hapus URL\n    text = url_pattern.sub('', text)\n\n    # Hapus hashtag\n    text = hashtag_pattern.sub('', text)\n\n    # Cek jika ada '--' dalam 40 karakter pertama\n    if '--' in text[:40]:\n        # Hapus header sebelum '--'\n        text = header_pattern.sub('', text).strip()\n\n    # Hapus frasa \"VIDEO:\" hingga titik\n    text = video_pattern.sub('', text)\n\n    # Hapus double space\n    text = double_space_pattern.sub(' ', text)\n\n    # Trim leading and trailing spaces\n    text = text.strip()\n\n    return text\n\n# Gunakan clean_text untuk membersihkan konteks\ncleaned_context = clean_text(context)\n\n# Lakukan inference menggunakan konteks yang sudah dibersihkan\nresult = question_answerer(question=question, context=cleaned_context)\n\n# Output hasil inference\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T02:32:27.434418Z","iopub.execute_input":"2024-09-08T02:32:27.434857Z","iopub.status.idle":"2024-09-08T02:32:28.004614Z","shell.execute_reply.started":"2024-09-08T02:32:27.434814Z","shell.execute_reply":"2024-09-08T02:32:28.003046Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"{'score': 0.3340967893600464, 'start': 744, 'end': 787, 'answer': '\"Family dynamics and personal relationships'}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}